#!/bin/python
import math
import numpy as np
import torch
import hw1_utils
import matplotlib as plt
from scipy.stats import norm, multivariate_normal

import faiss
from collections import Counter
import pandas as pd


################################# Problem 5 #################################
def k_means(X, k):
    """
    Implements Lloyd's algorithm.

    arguments:
    X -- n by d data matrix
    k -- integer, number of centers

    return:
    A matrix C of shape k by d with centers as its rows.
    """
    #Hint: You can use np.random.randn to initialize the centers randomly.
    #Hint: Implement auxiliary functions for recentering and for reassigning. Then repeat until no change.
    numPoints = X.shape[0]

    C = X[np.random.choice(numPoints,k, replace=False)]

    while True:
        euclidianDistances = np.linalg.norm(X[:, np.newaxis] - C, axis = 2)
        labels = np.argmin(euclidianDistances, axis = 1)

        Cnew = np.zeros_like(C)

        for i in range(k):
            clusterPoints = X[labels == i]
            if len(clusterPoints) > 0:
                Cnew[i] = clusterPoints.mean(axis = 0)
            else: 
                Cnew[i] = C[i]

        if np.all(Cnew == C):
            break

        C = Cnew

        return C


def get_purity_score(X, Y, C):
    """
    Computes the purity score for each cluster.

    arguments:
    X -- n by d data matrix
    Y -- n by 1 label vector
    C -- k by d center matrix

    return:
    Fraction of points with label matching their cluster's majority label.
    """
    numPoints = X.shape[0]
    numClusters = C.shape[0]

    euclidianDistances = np.linalg.norm(X[:, np.newaxis] - C, axis = 2)
    clusterAssignments = np.argmin(euclidianDistances, axis = 1)

    matchingCount = 0

    for cluster in range(numClusters):
        clusterIndices = np.where(clusterAssignments == cluster)[0]
        if len(clusterIndices) > 0:
            clusterLabels = Y[clusterIndices]
            mostCommonLabel = np.bincount(clusterLabels.flatten()).argmax()
            matchingCount += np.sum(clusterLabels == mostCommonLabel)

    return matchingCount / numPoints


################################# Problem 3 #################################
def gmm(X, k, epsilon=0.0001):
    """
    Computes the maximum likelihood Gaussian mixture model using expectation maximization algorithm.

    argument:
    X -- n by d data matrix
    k -- integer; number of Gaussian components
    epsilon -- improvement lower bound

    return:
    mu -- k by d matrix with centers as rows
    covars -- k by d matrix; row i is the diagonal of the covariance matrix of the i-th component
    weights -- k by 1 vector of probabilities over the Gaussian components
    """
    numSamples = X.shape[0]
    numFeatures = X.shape[1]
    
    means = k_means(X, k)  
    covars = np.ones((k, numFeatures)) * np.var(X, axis=0) 
    weights = np.ones(k) / k  
    
    logLikelihoodOld = -np.inf
    
    while True:
        responsibilities = np.zeros((numSamples, k))
        for i in range(k):
            rv = multivariate_normal(mean=means[i], cov=np.diag(covars[i]))
            responsibilities[:, i] = weights[i] * rv.pdf(X)
        
        responsibilities /= np.sum(responsibilities, axis=1, keepdims=True)  
        
        Nk = np.sum(responsibilities, axis=0) 
        weights = Nk / numSamples 
        
        means = (responsibilities.T @ X) / Nk[:, np.newaxis]  
        
        for i in range(k):
            diff = X - means[i]
            covars[i] = np.sum(responsibilities[:, i][:, np.newaxis] * (diff ** 2), axis=0) / Nk[i] 
        
        logLikelihood = np.sum(np.log(np.sum(responsibilities, axis=1)))
        
        if np.abs(logLikelihood - logLikelihoodOld) < epsilon:
            break
        logLikelihoodOld = logLikelihood
    
    return means, covars, weights

    


def gmm_predict(x, mu, covars, weights):
    """
    Computes the posterior probability of x having been generated by each of the k Gaussian components.

    arguments:
    x -- a single data point
    mu -- k by d matrix of centers
    covars -- a list k covariance matrices of shape (d, d)
    weights -- k by 1 vector of probabilities over the Gaussian components

    return:
    a k-vector that is the probability distribution of x having been generated by each of the Gaussian components.
    """
    numClusters = mu.shape[0]
    numFeatures = mu.shape[1]
    prob = np.zeros(numClusters)

    for i in range(numClusters):
        prob[i] = weights[i] * multivariate_normal.pdf(x, mean = mu[i], cov = np.diag(covars[i]))

    return prob / np.sum(prob)

################################# Problem 7 #################################


# ----- VectorDB Class -----
class VectorDB:
    def __init__(self, embedding_fn, metric="L2"):
        self.embedding_fn = embedding_fn
        self.metric = metric
        self.index = None
        self.labels = None

    def build(self, X, y):
        embeddings = self.embedding_fn(X)
        d = embeddings.shape[1]

        if self.metric == "L2":
            self.index = faiss.IndexFlatL2(d)
        elif self.metric == "cosine":
            faiss.normalize_L2(embeddings)  # Normalize each row to unit length
            self.index = faiss.IndexFlatIP(
                d
            )  # Inner product approximates cosine similarity
        else:
            raise ValueError("Unsupported metric! Use 'L2' or 'cosine'.")

        self.index.add(embeddings)
        self.labels = y

    def search(self, X_query, k=3):
        """
        Performs k-NN search using FAISS.

        :param X_query: Input query data (NumPy array of shape (N, ...)),
                    where N is the number of queries
        :param k: Number of nearest neighbors to retrieve.

        :return: indices (NumPy array of shape (N, k)) containing the indices of the k-nearest neighbors
             for each query sample.
        """
        pass

    def classify(self, X_query, k=3):
        """
        Performs k-NN classification by finding the k nearest neighbors of a query using self.search() and
        returning the most common label among them.

        :param X_query: Input query data (NumPy array of shape (N, D)),
                    where N is the number of queries and D is the feature dimension.
        :param k: Number of nearest neighbors to consider for classification.

        :return: NumPy array of shape (N,) containing the predicted class labels for each query.
        """
        pass


def evaluate(embedding_fn, metric, accuracy_results):
    name = embedding_fn.__name__ + "-" + metric
    vec_db = VectorDB(embedding_fn)
    vec_db.build(X_train, y_train)

    accuracy_results[name] = {}

    for k in [1, 3, 5, 10]:
        y_pred = vec_db.classify(X_test, k)
        accuracy_results[name][f"{k}-NN"] = np.mean(y_pred == y_test)


################################# Running script ############################


if __name__ == "__main__":
    # NOTE: The following code is a starting point to test your implementations
    # for this assignment. You will need to modify it according to your needs.
    # Remember to attach images generated for the corresponding questions
    # in the PDF submission in assignment `hw1` on Gradescope.
    
    # load Iris data
    print("Loading Iris Dataset")
    X, Xt, Y, Yt = hw1_utils.load_iris_data(0.8)

    if True:
        k_max = 25
        k_min = 2
        score = np.zeros(k_max - k_min)
        for k in range(k_min, k_max):
            C = k_means(X, k)
            score[k - k_min] = get_purity_score(Xt, Yt, C)
        hw1_utils.line_plot(score, min_k=k_min, output_file="purity_plot.pdf")
    if True:          
        k = 4
        C = k_means(X, k) 

        distances = np.linalg.norm(X[:, np.newaxis] - C, axis=2)
        clusterAssignments = np.argmin(distances, axis=1)

        clusterData = [X[clusterAssignments == i] for i in range(k)]

        hw1_utils.scatter_plot_2d_project(*clusterData, output_file="clusters_plot.pdf", ncol=3)
    if True:
        k_max = 11
        k_min = 2
        ll = np.zeros(k_max - k_min)
        for k in range(k_min, k_max):
            print(k)
            mu, variances, weights = gmm(X, k)

            likelihoods = np.zeros((X.shape[0], k))
            for j in range(k):
                likelihoods[:, j] = weights[j] * multivariate_normal.pdf(X, mean=mu[j], cov=np.diag(variances[j]))

            logLikelihood = np.sum(np.log(np.sum(likelihoods, axis=1)))  
            ll[k - k_min] = logLikelihood  
            
        hw1_utils.line_plot(ll, min_k=k_min, output_file="log_likelihood_plot.pdf")
    if True:
        n = X.shape[0]
        k = 4
        mu, variances, weights = gmm(X, k)

        R = np.zeros((n, k))
        for i in range(k):
            R[:, i] = weights[i] * multivariate_normal.pdf(X, mean=mu[i], cov=np.diag(variances[i]))

        R /= np.sum(R, axis=1, keepdims=True)

        A = np.zeros((n, k), dtype=np.bool_)
        for i in range(n):
            A[i, R[i, :].argsort()[-1:][::-1]] = True
        hw1_utils.gaussian_plot_2d_project(
            mu,
            variances,
            X[A[:, 0], :],
            X[A[:, 1], :],
            X[A[:, 2], :],
            X[A[:, 3], :],
            output_file="gmm_clusters.pdf",
        )
    elif True: 
        resnet18, transform = hw1_utils.get_resnet18()

        # Load CIFAR-10 subset
        X_train, y_train, X_test, y_test = hw1_utils.load_cifar10_subset(transform)

        faiss.omp_set_num_threads(1)  # Restrict CPU usage
        faiss.get_num_gpus = lambda: 0  # Disable FAISS GPU detectio

        identity_embedding = hw1_utils.identity_embedding
        def resnet18_embedding(x):
            return hw1_utils.resnet18_embedding(x, resnet18)

        accuracy_results = {}
        for embedding_fn in [identity_embedding, resnet18_embedding]:
            for metric in ["L2", "cosine"]:
                evaluate(embedding_fn, metric, accuracy_results)

        # Display results
        df_results = pd.DataFrame(accuracy_results)
        print("\n k-NN Accuracy Results:\n")
        pd.set_option("display.max_columns", None)
        print(df_results)

