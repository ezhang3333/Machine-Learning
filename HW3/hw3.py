import hw3_utils
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import hw3_utils as utils


def poly_gd(X, Y, lrate=0.01, num_iter=3000):
    """
    Gradient descent for polynomial regression.

    Parameters
    ----------
    X : array-like, shape (n_samples, d)
        Input data.
    Y : array-like, shape (n_samples, )
        Target values.
    lrate : float, optional
        Learning rate.
    num_iter : int, optional
        Number of iterations.

    Returns
    -------
    w : array-like, shape (dim_poly, )
        Model parameters in the order:
        (w0, w01, ..., w0d, w11, w12, ..., w1d, w22, w23, ..., wdd).
    """

    # Ensure X is 2D.
    X = np.asarray(X)
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    
    n = X.shape[0]
    d = X.shape[1]
    
    # Build feature matrix components in a list.
    features = []
    
    # Bias term: always include a column of ones.
    phi_bias = np.ones((n, 1))
    features.append(phi_bias)
    
    # Linear features: include if d > 0.
    if d > 0:
        phi_linear = X  # shape (n, d)
        features.append(phi_linear)
    
    # Quadratic features: for each i <= j.
    quad_list = []
    for i in range(d):
        for j in range(i, d):
            # Compute element-wise product and reshape as a column.
            quad_list.append((X[:, i] * X[:, j]).reshape(n, 1))
    if quad_list:
        phi_quad = np.hstack(quad_list)
        features.append(phi_quad)
    
    # Concatenate all feature components.
    Phi = np.hstack(features)
    D = Phi.shape[1]
    
    # Initialize weights to zero.
    w = np.zeros(D)
    
    # Optionally track risk to check monotonic decrease.
    prev_risk = np.inf
    
    # Gradient descent loop.
    for _ in range(num_iter):
        # Compute prediction.
        y_pred = Phi.dot(w)
        # Error vector.
        error = y_pred - Y
        # Compute risk as (1/(2n)) * sum(error^2)
        risk = np.mean(error**2) / 2
        
        # (Optional) Check risk decrease; for a small lrate, risk should decrease.
        # Uncomment the following line for debugging:
        # assert risk <= prev_risk + 1e-8, "Risk did not decrease: {} -> {}".format(prev_risk, risk)
        prev_risk = risk
        
        # Compute gradient: (1/n) * Phi^T (error)
        grad = (Phi.T.dot(error)) / n
        # Update weights.
        w = w - lrate * grad
        
    return w


def poly_normal(X, Y):
    """
    Polynomial regression using the normal equations.

    Parameters
    ----------
    X : array-like, shape (n_samples, d)
        Input data.
    Y : array-like, shape (n_samples, )
        Target values.
        
    Returns
    -------
    w : array-like, shape (dim_poly, )
        Model parameters in the order:
        (w0, w01, ..., w0d, w11, w12, ..., w1d, w22, w23, ..., wdd).
    """

    n = X.shape[0]
    d = X.shape[1]
    
    # Create bias term (column of ones)
    phi_bias = np.ones((n, 1))
    
    # Linear features remain unchanged.
    phi_linear = X  # shape (n, d)
    
    # Generate quadratic features: for each i <= j include x_i * x_j.
    quad_features = []
    for i in range(d):
        for j in range(i, d):
            quad_features.append((X[:, i] * X[:, j]).reshape(n, 1))
    phi_quad = np.hstack(quad_features) if quad_features else np.empty((n, 0))
    
    # Combine bias, linear, and quadratic features into one design matrix.
    Phi = np.hstack([phi_bias, phi_linear, phi_quad])
    
    # Solve the normal equations: w = (Phi^T * Phi)^{-1} * Phi^T * Y.
    # Here we use the pseudo-inverse to handle potential singularities.
    w = np.linalg.pinv(Phi) @ Y
    
    return w



def plot_poly():
    """
    Plot the curve generated by poly_normal() on the data from utils.load_reg_data().
    
    This function also plots the linear regression solution from utils.linear_normal() for comparison.
    Returns the plot as output.
    """
    # Assume these functions are imported from hw3_utils:
    # from hw3_utils import load_reg_data, poly_normal, linear_normal

    # Load regression data.
    X, Y = hw3_utils.load_reg_data()

    # Get the polynomial regression weights (for quadratic expansion).
    w_poly = poly_normal(X, Y)
    # Get the linear regression weights.
    w_lin = hw3_utils.linear_normal(X, Y)

    # Create a dense grid for smooth plotting.
    x_grid = np.linspace(X.min(), X.max(), 200)

    # For a one-dimensional input, the polynomial expansion is: [1, x, x^2].
    y_poly = w_poly[0] + w_poly[1] * x_grid + w_poly[2] * (x_grid ** 2)
    
    # Linear prediction: [1, x].
    y_lin = w_lin[0] + w_lin[1] * x_grid

    # Plotting.
    plt.figure(figsize=(8, 6))
    plt.scatter(X, Y, color='black', label='Data', alpha=0.7)
    plt.plot(x_grid, y_poly, color='red', label='Polynomial Regression')
    plt.plot(x_grid, y_lin, color='blue', linestyle='--', label='Linear Regression')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.title('Polynomial Regression vs. Linear Regression')
    plt.legend()
    plt.tight_layout()
    plt.show()

    # Return the current figure.
    return plt.gcf()



def poly_xor():
    """
    Train a polynomial regression model on the XOR dataset from hw3_utils.load_xor_data().
    Compare the linear classifier in hw3_utils.linear_normal() to the polynomial model in poly_normal().
    Plot the data points and the decision boundaries (contour lines) of both models.
    """

    # Load XOR dataset.
    X, Y = hw3_utils.load_xor_data()

    # Compute the models.
    w_lin = hw3_utils.linear_normal(X, Y)    # For linear model: [w0, w1, w2]
    w_poly = poly_normal(X, Y)     # For 2D input, order is: [w0, w01, w02, w11, w12, w22]

    # Create a grid for plotting decision boundaries.
    grid_vals = np.linspace(-2.5, 2.5, 200)
    X1, X2 = np.meshgrid(grid_vals, grid_vals)
    grid_points = np.c_[X1.ravel(), X2.ravel()]

    # Evaluate the linear model: f(x) = w0 + w1*x1 + w2*x2.
    f_lin = w_lin[0] + w_lin[1]*grid_points[:, 0] + w_lin[2]*grid_points[:, 1]
    f_lin = f_lin.reshape(X1.shape)

    # Evaluate the polynomial model:
    # For d = 2, the expanded model is:
    # f(x) = w0 + w01*x1 + w02*x2 + w11*x1^2 + w12*x1*x2 + w22*x2^2.
    f_poly = (w_poly[0] +
              w_poly[1] * grid_points[:, 0] +
              w_poly[2] * grid_points[:, 1] +
              w_poly[3] * grid_points[:, 0]**2 +
              w_poly[4] * grid_points[:, 0] * grid_points[:, 1] +
              w_poly[5] * grid_points[:, 1]**2)
    f_poly = f_poly.reshape(X1.shape)

    # Begin plotting.
    plt.figure(figsize=(8, 6))

    # Plot the XOR data points.
    # Class -1 will be green circles and class +1 will be blue triangles.
    neg = Y < 0
    pos = Y > 0
    plt.scatter(X[neg, 0], X[neg, 1], color='green', marker='o', s=100, label='Class -1')
    plt.scatter(X[pos, 0], X[pos, 1], color='blue', marker='^', s=100, label='Class +1')

    # Plot the decision boundaries as the contour f(x)=0.
    # Linear decision boundary (red dashed line).
    contour_lin = plt.contour(X1, X2, f_lin, levels=[0], colors='red', linestyles='dashed', linewidths=2)
    # Polynomial decision boundary (purple solid line).
    contour_poly = plt.contour(X1, X2, f_poly, levels=[0], colors='purple', linewidths=2)

    # Since contour plots do not directly support legends, we create proxy artists.
    from matplotlib.lines import Line2D
    lin_proxy = Line2D([0], [0], color='red', linestyle='dashed', linewidth=2,
                         label='Linear decision boundary')
    poly_proxy = Line2D([0], [0], color='purple', linestyle='-', linewidth=2,
                          label='Polynomial decision boundary')

    # Create proxy artists for the data points.
    neg_proxy = Line2D([0], [0], marker='o', color='w', markerfacecolor='green',
                         markersize=10, label='Class -1')
    pos_proxy = Line2D([0], [0], marker='^', color='w', markerfacecolor='blue',
                         markersize=10, label='Class +1')

    plt.legend(handles=[lin_proxy, poly_proxy, neg_proxy, pos_proxy])

    # Set labels and title.
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('Decision Boundaries on XOR Data: Linear vs. Polynomial')
    plt.xlim(grid_vals.min(), grid_vals.max())
    plt.ylim(grid_vals.min(), grid_vals.max())
    plt.tight_layout()
    plt.show()



def kernel_eval(x1, x2, ker):
    """
    Evaluate the kernel function between two input vectors x1 and x2.

    For the RBF kernel:
        k(x1, x2) = exp( -||x1 - x2||^2 / (2 * sigma^2) )
    where sigma is provided in ker['sigma'] or ker['param'].

    For the polynomial kernel:
        k(x1, x2) = (1 + x1 Â· x2)^p
    where p is provided in ker['p'] or ker['param'].

    Parameters
    ----------
    x1 : array-like, shape (d,)
        First input vector.
    x2 : array-like, shape (d,)
        Second input vector.
    ker : dict
        Dictionary containing the kernel type and its parameters.
        For example:
          - {'type': 'rbf', 'param': 1.0} or {'type': 'rbf', 'sigma': 1.0}
          - {'type': 'polynomial', 'param': 3} or {'type': 'poly', 'p': 3}

    Returns
    -------
    k_val : float
        Kernel evaluation result.
    """

    kernel_type = ker.get('type', '').lower()
    
    if kernel_type == 'rbf':
        # Use 'sigma' if provided, otherwise fall back to 'param'
        sigma = ker.get('sigma', ker.get('param', 1.0))
        diff = np.array(x1) - np.array(x2)
        norm_sq = np.dot(diff, diff)
        k_val = np.exp(-norm_sq / (2 * sigma**2))
    elif kernel_type in ['poly', 'polynomial']:
        # Use 'p' if provided, otherwise fall back to 'param'
        p = ker.get('p', ker.get('param', 2))
        k_val = (1 + np.dot(x1, x2)) ** p
    else:
        raise ValueError("Unsupported kernel type. Use 'rbf' or 'poly'.")
    
    return k_val



def kernel_gram(X, Xp, ker):
    """
    Compute the Gram (kernel) matrix between two sets of input vectors.

    Parameters
    ----------
    X : array-like, shape (n_samples, d)
        First set of input vectors.
    Xp : array-like, shape (m_samples, d)
        Second set of input vectors.
    ker : dict
        Dictionary specifying the kernel type and its parameters.
        For example:
            - {'type': 'rbf', 'sigma': 1.0}
            - {'type': 'poly', 'p': 3}

    Returns
    -------
    K : numpy.ndarray, shape (n_samples, m_samples)
        The Gram matrix, where K[i, j] = kernel_eval(X[i], Xp[j], ker).
    """

    # Ensure the inputs are numpy arrays.
    X = np.asarray(X)
    Xp = np.asarray(Xp)

    n = X.shape[0]
    m = Xp.shape[0]

    # Initialize the Gram matrix.
    K = np.zeros((n, m))

    # Compute each entry using kernel_eval.
    for i in range(n):
        for j in range(m):
            K[i, j] = kernel_eval(X[i], Xp[j], ker)

    return K



def kernel_ridge_regression(trn_x, trn_y, lam, ker):
    """
    Perform kernel ridge regression on the training data.

    Parameters
    ----------
    trn_x : array-like, shape (n_samples, d)
        Training input data.
    trn_y : array-like, shape (n_samples,)
        Training target values.
    lam : float
        Regularization parameter.
    ker : dict
        Dictionary specifying the kernel type and its parameters.
        For example:
            - {'type': 'rbf', 'sigma': 1.0}
            - {'type': 'poly', 'p': 3}

    Returns
    -------
    alpha : numpy.ndarray, shape (n_samples,)
        Coefficients for kernel ridge regression.
    """

    # Compute the Gram (kernel) matrix for the training data.
    K = kernel_gram(trn_x, trn_x, ker)
    
    # Regularize the Gram matrix by adding lambda times the identity matrix.
    n = K.shape[0]
    K_reg = K + lam * np.eye(n)
    
    # Solve the linear system: (K + lam*I) * alpha = trn_y.
    alpha = np.linalg.solve(K_reg, trn_y)
    
    return alpha



def kernel_predict(new_x, trn_x, alpha, ker):
    """
    Predict output values for new data using a kernel ridge regression model.

    Parameters
    ----------
    new_x : array-like, shape (n_new_samples, d)
        New input data.
    trn_x : array-like, shape (n_train_samples, d)
        Training input data.
    alpha : numpy.ndarray, shape (n_train_samples,)
        Learned coefficients from kernel ridge regression.
    ker : dict
        Dictionary specifying the kernel type and its parameters.
        For example:
            - {'type': 'rbf', 'sigma': 1.0}
            - {'type': 'poly', 'p': 3}

    Returns
    -------
    y_pred : numpy.ndarray, shape (n_new_samples,)
        Predicted output values for the new data.
    """
    # Compute the Gram matrix between new data and training data.
    K_new = kernel_gram(new_x, trn_x, ker)
    
    # Compute predictions by taking the dot product with alpha.
    y_pred = K_new.dot(alpha)
    
    return y_pred




def kernel_ridge_experiment(trn_x, trn_y, tst_x, tst_y, lambd, ker):
    """Run and evaluate a kernel ridge regression experiment.
    You do not need to implement this function.
    """
    alpha = kernel_ridge_regression(trn_x, trn_y, lambd, ker)

    # Compute training predictions and error.
    trn_pred = kernel_predict(trn_x, trn_x, alpha, ker)
    train_err = np.sum(trn_pred * trn_y <= 0) / len(trn_pred)

    # Compute test predictions and error.
    tst_pred = kernel_predict(tst_x, trn_x, alpha, ker)
    test_err = np.sum(tst_pred * tst_y <= 0) / len(tst_pred)

    print(
        "Ridge {} kernel with param {} and lambda {}: train error = {:.3f}, test error = {:.3f}".format(
            ker["type"], ker["param"], lambd, train_err, test_err
        )
    )

    boundary_true = utils.boundary_find(tst_x, tst_y)
    boundary_pred = utils.boundary_find(tst_x, tst_pred)

    if ker["type"] == "RBF":
        title_text = "Ridge: RBF kernel sigma = {}, lambda = {}".format(
            ker["param"], lambd
        )
    else:
        title_text = "Ridge: Poly kernel degree = {}, lambda = {}".format(
            ker["param"], lambd
        )

    outfn = "ridge-{}-{}-lambda{}.pdf".format(ker["type"], ker["param"], lambd)
    utils.boundary_plot(boundary_true, boundary_pred, tst_x, tst_y, outfn, title_text)
    return train_err, test_err


def plot_kernel_table():
    """Print a summary table of kernel ridge regression results.
    You do not need to implement this function.
    """
    np.random.seed(42)
    n = 100
    trn_x = np.random.uniform(-2, 2, (n, 2))
    trn_y = utils.genlabel(trn_x)

    tst_x = utils.gentestx(101)
    tst_y = utils.genlabel(tst_x)

    results = []  # Store errors for summary.
    lambdas = [0.1, 1.0]  # Regularization parameter values

    # Polynomial kernel experiments.
    for deg in [1, 4, 12]:
        ker = {"type": "polynomial", "param": deg}
        for lambd in lambdas:
            trn_err, tst_err = kernel_ridge_experiment(
                trn_x, trn_y, tst_x, tst_y, lambd, ker
            )
            results.append(
                {
                    "Method": "Ridge",
                    "Kernel": "Poly",
                    "Param": deg,
                    "Lambda": lambd,
                    "TrainError": trn_err,
                    "TestError": tst_err,
                }
            )

    # RBF kernel experiments.
    for sigma in [0.03, 0.3, 10]:
        ker = {"type": "RBF", "param": sigma}
        for lambd in lambdas:
            trn_err, tst_err = kernel_ridge_experiment(
                trn_x, trn_y, tst_x, tst_y, lambd, ker
            )
            results.append(
                {
                    "Method": "Ridge",
                    "Kernel": "RBF",
                    "Param": sigma,
                    "Lambda": lambd,
                    "TrainError": trn_err,
                    "TestError": tst_err,
                }
            )

    # Print summary results as a LaTeX table.
    print("\\begin{table}[h!]")
    print("\\centering")
    print("\\begin{tabular}{|c|c|c|c|c|c|}")
    print("\\hline")
    print("Method & Kernel & Param & Lambda & Train Error & Test Error \\\\ \\hline")
    for res in results:
        print(
            "{} & {} & {} & {} & {:.3f} & {:.3f} \\\\ \\hline".format(
                res["Method"],
                res["Kernel"],
                res["Param"],
                res["Lambda"],
                res["TrainError"],
                res["TestError"],
            )
        )
    print("\\end{tabular}")
    print("\\caption{Summary of kernel ridge regression results}")
    print("\\end{table}")



if __name__ == '__main__':
        plot_kernel_table()

        plot_poly()
        poly_xor()